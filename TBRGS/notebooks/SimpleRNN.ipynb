{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7378b7",
   "metadata": {},
   "source": [
    "# Traffic-based Route Guidance Solution\n",
    "\n",
    "## Import the Dependancies and Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34243419",
   "metadata": {
    "id": "34243419"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70369816",
   "metadata": {
    "id": "70369816"
   },
   "source": [
    "## Data Preprocessing & Analyzing\n",
    "\n",
    "1. Load the VicRoads Boroondara dataset (.csv)\n",
    "\n",
    "2. Clean and preprocess:\n",
    "    * Convert timestamps\n",
    "\n",
    "    * Handle missing values\n",
    "\n",
    "    * Normalize/scale traffic flow values\n",
    "\n",
    "3. Reshape for time-series forecasting (e.g., sequences of past 1-2 hours to predict next 15-min slot)\n",
    "\n",
    "### Import the Dataset for analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac413ca",
   "metadata": {
    "id": "eac413ca",
    "outputId": "f405c51b-e0cf-4cbe-b075-4dbc952cbbcf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_dataset = pd.read_csv('https://raw.githubusercontent.com/yasirulokesha/IntroToAI-Assignment-2B/refs/heads/main/TBRGS/data/raw/Scats%20Data%20October%202006.csv', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb1782",
   "metadata": {},
   "source": [
    "### Cluster and Make the Train and Test the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7_Vx9OK6Hzgw",
   "metadata": {
    "id": "7_Vx9OK6Hzgw"
   },
   "outputs": [],
   "source": [
    "# Create time mapping for V00‚ÄìV95 (15-minute intervals)\n",
    "time_labels = [f\"{h:02}:{m:02}:00\" for h in range(24) for m in range(0, 60, 15)]\n",
    "v_columns = [f\"V{str(i).zfill(2)}\" for i in range(96)]\n",
    "time_map = dict(zip(v_columns, time_labels))\n",
    "\n",
    "# Melt the V columns into long format\n",
    "df_melted = main_dataset.melt(\n",
    "    id_vars=[\"SCATS Number\", \"Location\", \"Date\"],\n",
    "    value_vars=v_columns,\n",
    "    var_name=\"time_code\",\n",
    "    value_name=\"volume\"\n",
    ")\n",
    "\n",
    "# Map time codes to actual time strings\n",
    "df_melted['time_str'] = df_melted['time_code'].map(time_map)\n",
    "\n",
    "# Combine Date and Time into a full timestamp\n",
    "df_melted['timestamp'] = pd.to_datetime(df_melted['Date'] + ' ' + df_melted['time_str'], dayfirst=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_melted = df_melted.rename(columns={\n",
    "    \"SCATS Number\": \"site_id\",\n",
    "    \"Location\": \"location\"\n",
    "})\n",
    "\n",
    "# Select and reorder important columns\n",
    "df_traffic = df_melted[[\"site_id\", \"location\", \"timestamp\", \"volume\"]]\n",
    "\n",
    "# üîÅ AGGREGATE: Sum volume over all directions at each site per timestamp\n",
    "final_dataset = df_melted.groupby(['site_id', 'timestamp'])['volume'].sum().reset_index()\n",
    "\n",
    "sites = final_dataset['site_id'].unique()\n",
    "\n",
    "final_dataset.to_csv('../data/processed/processed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca94c7",
   "metadata": {},
   "source": [
    "### Scaling the dataset using MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c2600",
   "metadata": {},
   "source": [
    "### Breakdown the dataset for unique sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb038e7",
   "metadata": {},
   "source": [
    "### Data processing for ML\n",
    "\n",
    "#### Create Time Series Sequences for Training and Testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a35749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for creating sequences\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd61da",
   "metadata": {},
   "source": [
    "### Make the unique sequences for different sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_sequences(data_set):\n",
    "    x, y = create_sequences(data_set, 96)\n",
    "    x = x.reshape(-1, 96, 1)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e44ab",
   "metadata": {},
   "source": [
    "### Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers using IQR\n",
    "def replace_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)  # First quartile (25th percentile)\n",
    "    Q3 = df[column].quantile(0.75)  # Third quartile (75th percentile)\n",
    "    IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Replace outliers with the median\n",
    "    median = df[column].median()\n",
    "    df[column] = df[column].apply(lambda x: median if x < lower_bound or x > upper_bound else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b0f53",
   "metadata": {},
   "source": [
    "## - Generating the models and save for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in sites:\n",
    "    print(id)\n",
    "    site_data = final_dataset[final_dataset['site_id'] == id].sort_values(\"timestamp\")\n",
    "    \n",
    "    replace_outliers_iqr(site_data, 'volume')\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    site_data['volume'] = scaler.fit_transform(site_data[['volume']])\n",
    "    \n",
    "    site_data = np.array(site_data['volume'])\n",
    "    \n",
    "    x, y = generating_sequences(site_data)\n",
    "    \n",
    "    # Split into training and testing\n",
    "    split = int(len(x) * 0.8)\n",
    "    X_train, X_test = x[:split], x[split:]\n",
    "    Y_train, Y_test = y[:split], y[split:]\n",
    "    \n",
    "    # Define the LSTM model\n",
    "    model = Sequential([\n",
    "        Input(shape=(96, 1)),\n",
    "        SimpleRNN(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        epochs=40,\n",
    "        batch_size=96\n",
    "        # callbacks = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"../src/models/RNN_model/models/rnn_model_site_{id}.keras\")  # New format\n",
    "\n",
    "    joblib.dump(scaler, f\"../src/models/RNN_model/scalers/scaler_site_{id}.save\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Model and scaler saved for site {id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54596e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers using IQR\n",
    "replace_outliers_iqr(final_dataset, 'volume')\n",
    "\n",
    "for id in sites:\n",
    "    # Load the model and scaler\n",
    "    model = load_model(f\"../models/SimpleRNN_model/models/simplernn_model_site_{id}.keras\")\n",
    "    scaler = joblib.load(f\"../models/SimpleRNN_model/scalers/scaler_site_{id}.save\")\n",
    "    print(f\"\\nüö¶ Loading model and scaler for site {id}...\")\n",
    "    \n",
    "    # Filter data for this site\n",
    "    site_data = final_dataset[final_dataset['site_id'] == id].sort_values(\"timestamp\")\n",
    "    \n",
    "    site_data['volume'] = scaler.transform(site_data[['volume']])\n",
    "    \n",
    "    # Predictions for the next time step\n",
    "    last_site_data = np.array(site_data['volume'][-96:]).reshape(-1, 96, 1)\n",
    "   \n",
    "    # Make predictions on the test set\n",
    "    last_test_site_data = np.array(site_data['volume'][:97])\n",
    "    \n",
    "    testX, testY = generating_sequences(last_test_site_data)\n",
    "    \n",
    "    testX = testX.reshape(-1, 96, 1)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(testX)\n",
    "\n",
    "    # Reverse scaling if necessary\n",
    "    y_test_actual = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "    y_pred_actual = scaler.inverse_transform(y_pred)\n",
    "\n",
    "    smape = 100 * abs(y_pred_actual - y_test_actual) / ((abs(y_pred_actual) + abs(y_test_actual)) / 2)\n",
    "    accuracy = 100 - smape\n",
    "    accuracy = np.mean(accuracy)\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    pred_scaled = model.predict(last_site_data)\n",
    "    pred_volume = scaler.inverse_transform(pred_scaled)[0][0]\n",
    "\n",
    "    # print(f\"Predicted volume for the next time step: {pred_volume:.2f}\")\n",
    "    # print(pred_scaled)\n",
    "    print(pred_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef4cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_based_guidance_solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
